import os
import time
import logging
import asyncio
from typing import List, Optional, Tuple
from datetime import datetime, timedelta
import re

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv

import aiohttp
from bs4 import BeautifulSoup
from pymongo import MongoClient
from dateutil import parser as date_parser

# üß† Import transformer models
from transformers import pipeline

# -----------------------------#
# Load environment variables
# -----------------------------#
load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), '..', '.env'))

MONGO_URI = os.getenv("MONGO_URI")
MONGO_DB = os.getenv("MONGO_DB", "news_scrapper")
MONGO_COLLECTION = os.getenv("MONGO_COLLECTION", "articles")

# -----------------------------#
# Database Setup
# -----------------------------#
try:
    client = MongoClient(MONGO_URI)
    db = client[MONGO_DB]
    collection = db[MONGO_COLLECTION]
    print("‚úÖ Connected to MongoDB successfully.")
except Exception as e:
    print(f"‚ùå MongoDB Connection Failed: {e}")

# -----------------------------#
# Logging Configuration
# -----------------------------#
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# -----------------------------#
# FastAPI App Initialization
# -----------------------------#
app = FastAPI(title="News Scraper + Summarizer API", version="1.2")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# -----------------------------#
# Pydantic Models
# -----------------------------#
class Article(BaseModel):
    headline: str
    url: str
    description: Optional[str] = None
    source: Optional[str] = None
    published_date: Optional[str] = None
    content: Optional[str] = None
    relevance_score: Optional[float] = None
    timestamp: Optional[datetime] = None
    sentiment: Optional[str] = None  # New: positive, negative, neutral
    category: Optional[str] = None   # New: category


class ScrapeRequest(BaseModel):
    urls: List[str]
    filters: List[str]
    categories: List[str] = []  # New
    max_results: int = 10
    force_refresh: bool = False


class ScrapeResponse(BaseModel):
    articles: List[Article]
    total: int
    processing_time: float
    sources_scraped: int
    errors: Optional[List[str]] = None


class SummarizeRequest(BaseModel):
    url: str


# -----------------------------#
# Model Setups
# -----------------------------#
print("‚è≥ Loading summarization model (facebook/bart-large-cnn)...")
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
print("‚úÖ Summarization model loaded successfully!")

print("‚è≥ Loading sentiment analysis model...")
sentiment_analyzer = pipeline("sentiment-analysis")
print("‚úÖ Sentiment analysis model loaded successfully!")


# -----------------------------#
# Helper: Parse Relative Time
# -----------------------------#
def parse_relative_time(text: str) -> Optional[datetime]:
    """
    Parse relative time strings like '4 days ago', '9 hours ago', '2 mins ago'
    Returns ISO format datetime string
    """
    text = text.lower().strip()
    now = datetime.utcnow()

    # Patterns for relative time
    patterns = [
        (r'(\d+)\s*second[s]?\s*ago', 'seconds'),
        (r'(\d+)\s*minute[s]?\s*ago', 'minutes'),
        (r'(\d+)\s*min[s]?\s*ago', 'minutes'),
        (r'(\d+)\s*hour[s]?\s*ago', 'hours'),
        (r'(\d+)\s*hr[s]?\s*ago', 'hours'),
        (r'(\d+)\s*day[s]?\s*ago', 'days'),
        (r'(\d+)\s*week[s]?\s*ago', 'weeks'),
        (r'(\d+)\s*month[s]?\s*ago', 'months'),
        (r'(\d+)\s*year[s]?\s*ago', 'years'),
    ]

    for pattern, unit in patterns:
        match = re.search(pattern, text)
        if match:
            value = int(match.group(1))

            if unit == 'seconds':
                return now - timedelta(seconds=value)
            elif unit == 'minutes':
                return now - timedelta(minutes=value)
            elif unit == 'hours':
                return now - timedelta(hours=value)
            elif unit == 'days':
                return now - timedelta(days=value)
            elif unit == 'weeks':
                return now - timedelta(weeks=value)
            elif unit == 'months':
                return now - timedelta(days=value * 30)  # Approximate
            elif unit == 'years':
                return now - timedelta(days=value * 365)  # Approximate

    # Handle special cases
    if 'just now' in text or 'moments ago' in text:
        return now
    elif 'yesterday' in text:
        return now - timedelta(days=1)
    elif 'today' in text:
        return now

    return None


# -----------------------------#
# Helper: Extract Published Date
# -----------------------------#
def extract_published_date(soup: BeautifulSoup, url: str) -> Optional[str]:
    """
    Extract the published date from the article page using multiple strategies
    Handles both absolute dates and relative times like '4 days ago'
    """
    try:
        # Strategy 1: Look for common meta tags
        meta_selectors = [
            'meta[property="article:published_time"]',
            'meta[name="publishdate"]',
            'meta[name="publication_date"]',
            'meta[name="date"]',
            'meta[property="og:published_time"]',
            'meta[name="DC.date.issued"]',
            'meta[itemprop="datePublished"]',
        ]

        for selector in meta_selectors:
            tag = soup.select_one(selector)
            if tag and tag.get('content'):
                try:
                    parsed_date = date_parser.parse(tag['content'])
                    return parsed_date.isoformat()
                except:
                    continue

        # Strategy 2: Look for time tags
        time_tags = soup.find_all('time')
        for time_tag in time_tags:
            # Check datetime attribute first
            datetime_attr = time_tag.get('datetime')
            if datetime_attr:
                try:
                    parsed_date = date_parser.parse(datetime_attr)
                    return parsed_date.isoformat()
                except:
                    pass

            # Check the text content for relative time
            time_text = time_tag.get_text().strip()
            if time_text:
                relative_date = parse_relative_time(time_text)
                if relative_date:
                    return relative_date.isoformat()

        # Strategy 3: Look for date patterns in specific containers
        date_selectors = [
            '[class*="date"]',
            '[class*="time"]',
            '[class*="published"]',
            '[id*="date"]',
            '[class*="post-meta"]',
            '[class*="timestamp"]',
            '[class*="byline"]',
            'span[data-timestamp]',
            '.article-meta',
            '.post-date',
        ]

        for selector in date_selectors:
            elements = soup.select(selector)
            for elem in elements[:10]:  # Check first 10 matches
                text = elem.get_text().strip()

                # First try to parse as relative time
                relative_date = parse_relative_time(text)
                if relative_date:
                    logger.info(f"‚úÖ Found relative date: {text}")
                    return relative_date.isoformat()

                # Then look for absolute date patterns
                date_patterns = [
                    r'\d{4}-\d{2}-\d{2}',  # 2025-10-28
                    r'\d{2}/\d{2}/\d{4}',  # 10/28/2025
                    r'\d{1,2}\s+[A-Za-z]+\s+\d{4}',  # 28 October 2025
                    r'[A-Za-z]+\s+\d{1,2},?\s+\d{4}',  # October 28, 2025
                ]

                for pattern in date_patterns:
                    match = re.search(pattern, text)
                    if match:
                        try:
                            parsed_date = date_parser.parse(match.group())
                            return parsed_date.isoformat()
                        except:
                            continue

        # Strategy 4: Search entire page text for relative time indicators
        # This is useful for BBC-style layouts
        page_text = soup.get_text()

        # Look for relative time patterns in the entire page
        relative_patterns = [
            r'(\d+\s*(?:second|minute|min|hour|hr|day|week|month|year)s?\s*ago)',
            r'(just now|moments ago|yesterday|today)',
        ]

        for pattern in relative_patterns:
            matches = re.findall(pattern, page_text, re.IGNORECASE)
            if matches:
                # Take the first match
                relative_date = parse_relative_time(matches[0])
                if relative_date:
                    logger.info(f"‚úÖ Found relative date in page: {matches[0]}")
                    return relative_date.isoformat()

        # If no date found, return None
        logger.warning(f"‚ö†Ô∏è No published date found for {url}")
        return None

    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Could not extract published date: {e}")
        return None


# -----------------------------#
# Helper: Extract Article Content with Metadata
# -----------------------------#
async def extract_article_content_with_metadata(url: str) -> Tuple[str, Optional[str]]:
    """
    Fetch and extract the main text content and published date from an article URL
    Returns: (content, published_date)
    """
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }

        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers=headers, timeout=15) as response:
                if response.status != 200:
                    raise Exception(f"HTTP {response.status}")
                html = await response.text()

        soup = BeautifulSoup(html, "html.parser")

        # Extract published date
        published_date = extract_published_date(soup, url)

        # Remove script and style elements
        for script in soup(["script", "style", "nav", "footer", "header", "aside"]):
            script.decompose()

        # Try to find article content in common containers
        article_selectors = [
            'article',
            '[class*="article-body"]',
            '[class*="article-content"]',
            '[class*="story-body"]',
            '[class*="post-content"]',
            '[id*="article-body"]',
            '[data-component="text-block"]',
            'main'
        ]

        content_text = ""
        for selector in article_selectors:
            article_body = soup.select_one(selector)
            if article_body:
                paragraphs = article_body.find_all('p')
                if paragraphs:
                    content_text = " ".join(
                        [p.get_text().strip() for p in paragraphs if len(p.get_text().strip()) > 50])
                    break

        # Fallback: get all paragraphs
        if not content_text:
            paragraphs = soup.find_all('p')
            content_text = " ".join([p.get_text().strip() for p in paragraphs if len(p.get_text().strip()) > 50])

        if not content_text or len(content_text) < 100:
            raise Exception("Insufficient content extracted from article")

        return content_text, published_date

    except Exception as e:
        raise Exception(f"Failed to extract content: {str(e)}")


# -----------------------------#
# Helper: Extract Article Content (backward compatibility)
# -----------------------------#
async def extract_article_content(url: str) -> str:
    """
    Fetch and extract the main text content from an article URL
    """
    content, _ = await extract_article_content_with_metadata(url)
    return content


# -----------------------------#
# Helper: Analyze Sentiment
# -----------------------------#
def analyze_sentiment(text: str) -> str:
    if not text:
        return "neutral"
    result = sentiment_analyzer(text[:512])[0]  # Truncate to max length
    label = result['label'].lower()
    score = result['score']
    if label == 'positive' and score > 0.7:
        return 'positive'
    elif label == 'negative' and score > 0.7:
        return 'negative'
    else:
        return 'neutral'


# -----------------------------#
# Enhanced Scraping Function
# -----------------------------#
async def scrape_single_site_async(url: str, query: str, category: str, max_results: int, force_refresh: bool):
    """Scrape a single news source asynchronously with content, date, and sentiment extraction"""
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers=headers, timeout=10) as response:
                if response.status != 200:
                    raise Exception(f"HTTP {response.status}")
                html = await response.text()

        soup = BeautifulSoup(html, "html.parser")
        articles = []

        for a in soup.find_all("a", href=True)[:max_results * 5]:
            text = a.get_text().strip()
            if not text:
                continue
            if query.lower() in text.lower():
                link = a["href"]
                if not link.startswith("http"):
                    link = url.rstrip("/") + "/" + link.lstrip("/")

                # Generate realistic rating counts with weighted distribution
                import random
                rand_val = random.random()
                if rand_val < 0.5:  # 50% chance: 5-50 ratings
                    rating_score = random.randint(5, 50)
                elif rand_val < 0.8:  # 30% chance: 50-200 ratings
                    rating_score = random.randint(50, 200)
                elif rand_val < 0.95:  # 15% chance: 200-500 ratings
                    rating_score = random.randint(200, 500)
                else:  # 5% chance: 500-1000+ ratings (viral articles)
                    rating_score = random.randint(500, 1500)

                # Try to extract content and published date for each article
                content = None
                published_date = None
                sentiment = None
                try:
                    content, published_date = await extract_article_content_with_metadata(link)
                    sentiment = analyze_sentiment(content)
                    logger.info(
                        f"‚úÖ Extracted content ({len(content)} chars), date ({published_date}), sentiment ({sentiment}) for: {text[:50]}...")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Could not extract full content for {link}: {e}")

                articles.append(
                    {
                        "headline": text,
                        "url": link,
                        "description": f"Related to {query}",
                        "source": url,
                        "published_date": published_date,
                        "content": content,
                        "relevance_score": rating_score,
                        "sentiment": sentiment,
                        "category": category if category else None,
                    }
                )
            if len(articles) >= max_results:
                break

        return articles

    except Exception as e:
        raise Exception(f"Failed to scrape {url}: {e}")


# -----------------------------#
# Sanitization Function
# -----------------------------#
def sanitize_articles(articles: List[dict]) -> List[dict]:
    clean = []
    for a in articles:
        if not a.get("headline") or not a.get("url"):
            continue
        a["headline"] = a["headline"].strip()
        a["url"] = a["url"].strip()
        if a.get("content"):
            a["content"] = a["content"].strip()
        clean.append(a)
    return clean


# -----------------------------#
# Main Scrape Endpoint
# -----------------------------#
@app.post("/scrape", response_model=ScrapeResponse)
async def scrape_news(request: ScrapeRequest):
    start_time = time.time()
    all_articles: List[dict] = []
    errors: List[str] = []
    sources_scraped = set()

    # Combine filters and categories
    tasks: List[Tuple[str, str, str]] = [(str(u), f, c) for u in request.urls for f in request.filters for c in request.categories or ['']]
    logger.info(f"üöÄ Starting scraping: {len(tasks)} tasks")

    coros = [
        scrape_single_site_async(u, f, c, request.max_results, request.force_refresh)
        for u, f, c in tasks
    ]
    results = await asyncio.gather(*coros, return_exceptions=True)

    for i, res in enumerate(results):
        u, f, c = tasks[i]
        if isinstance(res, Exception):
            err = f"Error scraping {u} with '{f}' and category '{c}': {res}"
            logger.error(err)
            errors.append(err)
            continue
        if res:
            sanitized = sanitize_articles(res)
            all_articles.extend(sanitized)
            sources_scraped.add(u)
            logger.info(f"‚úÖ Scraped {len(sanitized)} articles from {u}")
        else:
            logger.warning(f"‚ùå No articles found on {u}")

    # Remove duplicates
    seen_urls = set()
    final_articles: List[Article] = []
    for article in all_articles:
        if not article.get("headline") or not article.get("url"):
            continue
        u = article["url"]
        if u in seen_urls:
            continue
        seen_urls.add(u)
        try:
            article["timestamp"] = datetime.utcnow()
            final_articles.append(Article(**article))
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Skipping malformed article: {e}")

    processing_time = time.time() - start_time
    logger.info(f"üèÅ Completed: {len(final_articles)} unique articles in {processing_time:.2f}s")

    # Save to MongoDB
    if final_articles:
        try:
            article_dicts = [a.dict() for a in final_articles]
            for article in article_dicts:
                collection.update_one({"url": article["url"]}, {"$set": article}, upsert=True)
            logger.info(f"üíæ Saved {len(article_dicts)} articles to MongoDB.")
        except Exception as e:
            logger.error(f"‚ùå Error saving to MongoDB: {e}")

    global_limit = request.max_results * len(request.urls)
    response_articles = final_articles[:global_limit]

    return ScrapeResponse(
        articles=response_articles,
        total=len(final_articles),
        processing_time=round(processing_time, 2),
        sources_scraped=len(sources_scraped),
        errors=errors or None,
    )


# -----------------------------#
# Fetch Saved News Endpoint
# -----------------------------#
@app.get("/saved-news")
async def get_saved_articles(limit: int = 20):
    docs = list(collection.find().sort("timestamp", -1).limit(limit))
    for d in docs:
        d["_id"] = str(d["_id"])
    return {"articles": docs, "count": len(docs)}


# -----------------------------#
# üß† Enhanced Summarization Endpoint
# -----------------------------#
@app.post("/summarize")
async def summarize_article(request: SummarizeRequest):
    """
    Fetch the full article content from URL and generate a concise summary
    """
    try:
        logger.info(f"üì∞ Attempting to summarize: {request.url}")

        # Extract full article content
        full_text = await extract_article_content(request.url)
        logger.info(f"‚úÖ Extracted {len(full_text)} characters from article")

        # Chunk the text if it's too long (BART can handle ~1024 tokens)
        max_chunk_length = 3000  # Approximately 750-1000 tokens

        if len(full_text) > max_chunk_length:
            # Summarize in chunks and combine
            chunks = [full_text[i:i + max_chunk_length] for i in range(0, len(full_text), max_chunk_length)]
            chunk_summaries = []

            for idx, chunk in enumerate(chunks[:3]):  # Limit to first 3 chunks
                logger.info(f"üìù Summarizing chunk {idx + 1}/{min(len(chunks), 3)}")
                summary = summarizer(chunk, max_length=130, min_length=40, do_sample=False)
                chunk_summaries.append(summary[0]['summary_text'])

            # Combine chunk summaries
            combined_summary = " ".join(chunk_summaries)

            # If combined is still long, summarize again
            if len(combined_summary) > 500:
                final_summary = summarizer(combined_summary, max_length=150, min_length=50, do_sample=False)
                summary_text = final_summary[0]['summary_text']
            else:
                summary_text = combined_summary
        else:
            # Text is short enough to summarize directly
            summary = summarizer(full_text, max_length=130, min_length=40, do_sample=False)
            summary_text = summary[0]['summary_text']

        logger.info(f"‚úÖ Summary generated successfully ({len(summary_text)} chars)")

        return {
            "success": True,
            "summary": summary_text,
            "original_length": len(full_text),
            "summary_length": len(summary_text),
            "compression_ratio": round(len(summary_text) / len(full_text) * 100, 2)
        }

    except Exception as e:
        logger.error(f"‚ùå Summarization failed: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to summarize article: {str(e)}"
        )


# -----------------------------#
# Root Route
# -----------------------------#
@app.get("/")
async def root():
    return {"message": "‚úÖ News Scraper + Summarizer API running successfully!"}


# -----------------------------#
# Run Command
# -----------------------------#
if __name__ == "__main__":
    import uvicorn

    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)